# This reads the simulations outcomes generated by 1_script.jl and computes relevant statistics
# this file is loaded by generate_plots_x.jl

# load dependencies
using TOML
using JLD2
using StatsBase
using DataStructures
using Random
using ArgParse

# load files and functions 
macro names(arg...) string.(arg) end
include(joinpath(pwd(),"analysis.jl"))
include(joinpath(pwd(),"rl_agents.jl"))
include(joinpath(pwd(),"nash.jl"))

# tolerance on convergence of policies
const rtol = 0.001f0
# tolerance on probabilities (off-path messages, partitional, effective messages)
const ptol = 0.001f0
# tolerance on gamma-nash (gamma < gtol)
const gtol = 0.01f0

# functions

function parse_commandline(default_input, default_step_bias)
    arg_settings = ArgParseSettings(allow_ambiguous_opts=true)
    @add_arg_table! arg_settings begin
        # GAME SETTINGS
        "--in_dir", "-i"
            arg_type = String
            help = "input directory"
            default = default_input
        "--step_bias"
            arg_type = Float32
            help = "space between points in [0.0,0.5]"
            default = default_step_bias
    end
    parsed_args = parse_args(arg_settings)
    return parsed_args
end

function extract_data(config, results, extracted_data)
	# extract and process data from data structures

	# read from files, some variables must be global
	global bias = Float32(config["bias"])
	global reward_matrix_s, reward_matrix_r = gen_reward_matrix()

	babbling_actions = argmax_(reward_matrix_r*p_t)
	babbling_reward_s::Float32 = mean(p_t'reward_matrix_s[a,:] for a in babbling_actions)
	babbling_reward_r::Float32 =  mean(p_t'reward_matrix_r[a,:] for a in babbling_actions)

	n_max_episodes = config["n_max_episodes"] 	
	Q_s = results["Q_s"]
	Q_r = results["Q_r"]
	n_episodes = results["n_episodes"]

	# compute is_converged bool and frequence
    is_converged = n_episodes .< n_max_episodes
    n_converged = count(is_converged)
    n_converged == n_simulations || exit() # failsafe

	# preallocate array
    policy_s = Array{Float32,3}(undef, n_states, n_messages, n_converged)
    policy_r = Array{Float32,3}(undef, n_messages, n_actions, n_converged)
    induced_actions = Array{Float32,3}(undef, n_states, n_actions, n_converged)
    expected_reward_s = Array{Float32,1}(undef, n_converged)
    expected_reward_r = Array{Float32,1}(undef, n_converged)
    absolute_error_s = Array{Float32,1}(undef, n_converged)
    absolute_error_r = Array{Float32,1}(undef, n_converged)
    max_absolute_error = Array{Float32,1}(undef, n_converged)
    mutual_information = Array{Float32,1}(undef, n_converged)
    residual_variance = Array{Float32,1}(undef, n_converged)
    n_on_path_messages = Array{Int64,1}(undef, n_converged)
    max_mass_on_suboptim_s = Array{Float32,1}(undef, n_converged)
    max_mass_on_suboptim_r = Array{Float32,1}(undef, n_converged)
    is_partitional = Array{Bool,1}(undef, n_converged)
    n_effective_messages = Array{Int64,1}(undef, n_converged)
    max_max_mass_on_suboptim = Array{Float32,1}(undef, n_converged)
	# compute metrics of interest for converged sessions
	Threads.@threads for z in 1:n_converged
		is_converged[z] == true || continue
        # get policies at convergence
       	policy_s_ = get_policy(Q_s[:,:,z], max(temp0_s*lambda_s^(n_episodes[z]-1),1f-30))
        policy_r_ = get_policy(Q_r[:,:,z], max(temp0_r*lambda_r^(n_episodes[z]-1),1f-30))
        policy_s[:,:,z], policy_r[:,:,z] = order_policies(policy_s_, policy_r_)
        # compute induced actions at convergence
        induced_actions[:,:,z] = get_induced_actions(policy_s_, policy_r_)    
        # compute (ex-ante) expected rewards at convergence
        expected_reward_s[z], expected_reward_r[z] = get_expected_rewards(induced_actions[:,:,z])
        # compute best response to opponent's policy at convergence
        optimal_policy_s = get_best_reply_s(policy_r_)
        optimal_policy_r = get_best_reply_r(policy_s_)
        # compute expected rewards by best responding to opponent
        optimal_reward_s, _ = get_expected_rewards(optimal_policy_s, policy_r_)
        _, optimal_reward_r = get_expected_rewards(policy_s_, optimal_policy_r)
        # compute absolute expected error by (possibly) not best responding to opponent
        absolute_error_s[z] = optimal_reward_s - expected_reward_s[z] 
        absolute_error_r[z] = optimal_reward_r - expected_reward_r[z]
		max_absolute_error[z] = max(absolute_error_s[z], absolute_error_r[z])
        # compute communication metrics
        mutual_information[z] = get_mutual_information(policy_s_)
        residual_variance[z] = get_residual_variance(policy_s_)
        # get on path messages
        off_path_messages = get_off_path_messages(policy_s_)
		n_on_path_messages[z] = n_messages - count(off_path_messages)
        # compute maximum mass on suboptim messages (actions) across states (messages)
        mass_on_suboptim_s = get_mass_on_suboptim(policy_s_, optimal_policy_s)
        mass_on_suboptim_r = get_mass_on_suboptim(policy_r_, optimal_policy_r)
        max_mass_on_suboptim_s[z] = maximum(mass_on_suboptim_s)
    	max_mass_on_suboptim_r[z] = maximum(mass_on_suboptim_r[.!off_path_messages])
		# check if is a nash
        max_max_mass_on_suboptim[z] = max(max_mass_on_suboptim_s[z], max_mass_on_suboptim_r[z])
        # check if policy is partitional
        is_partitional[z] = ispartitional(policy_s_)
        # count number of messages that have no synonyms
        n_effective_messages[z] = count(get_effective_messages(policy_s_[:,.!off_path_messages]))
    end

	i = findfirst(set_biases .== bias)
	extracted_data["policy_s"][i] = policy_s
	extracted_data["policy_r"][i] = policy_r
    extracted_data["induced_actions"][i] = induced_actions
	extracted_data["n_episodes"][i] = n_episodes
	extracted_data["is_converged"][i] = is_converged
	extracted_data["expected_reward_s"][i] = expected_reward_s
	extracted_data["expected_reward_r"][i] = expected_reward_r
	extracted_data["mutual_information"][i] = mutual_information
	extracted_data["residual_variance"][i] = residual_variance
	extracted_data["absolute_error_s"][i] = absolute_error_s
	extracted_data["absolute_error_r"][i] = absolute_error_r
	extracted_data["max_absolute_error"][i] = max_absolute_error
	extracted_data["n_on_path_messages"][i] = n_on_path_messages
	extracted_data["max_mass_on_suboptim_s"][i] = max_mass_on_suboptim_s
	extracted_data["max_mass_on_suboptim_r"][i] = max_mass_on_suboptim_r
	extracted_data["max_max_mass_on_suboptim"][i] = max_max_mass_on_suboptim
	extracted_data["is_partitional"][i] = is_partitional
	extracted_data["n_effective_messages"][i] = n_effective_messages
	extracted_data["babbling_reward_s"][i] = babbling_reward_s
	extracted_data["babbling_reward_r"][i] = babbling_reward_r
end

function get_equilibria(n_steps)
	# Compute sender-prefered equilibria for bias in range(0,0.5,n_steps)
	# only supports n_messages = n_states
	best_nash = DefaultDict(() -> Array{Float32,1}(undef,n_steps))
	set_nash = DefaultDict(() -> Array{Any,1}(undef,n_steps))
	set_biases_ = range(0f0,0.5f0,n_steps)
	for i in 1:n_steps
	    print("\rComputing benchmark $i/$n_steps")
	    flush(stdout)
		global bias = set_biases_[i]
		global reward_matrix_s, reward_matrix_r = gen_reward_matrix()
		best_nash_i = get_best_nash()
		set_nash_i = get_monotone_partitional_equilibria()
		best_nash["expected_reward_s"][i] = best_nash_i["best_expected_reward_s"]
		best_nash["expected_reward_r"][i] = best_nash_i["best_expected_reward_r"]
		best_nash["mutual_information"][i] = best_nash_i["best_mutual_information"]
		best_nash["residual_variance"][i] = best_nash_i["best_residual_variance"]
		set_nash["expected_reward_s"][i] = set_nash_i["expected_reward_s"]
		set_nash["expected_reward_r"][i] = set_nash_i["expected_reward_r"]
		set_nash["mutual_information"][i] = set_nash_i["mutual_information"]
		set_nash["residual_variance"][i] = set_nash_i["residual_variance"]
		set_nash["induced_actions"][i] = set_nash_i["induced_actions"]
	end
	println()
	return set_nash, best_nash
end


function read_data(dir)
	# main loop, read files in subdirs, extract and process data, save to extracted_data
	subdirs = readdir(dir, join=true)	
	subdirs = subdirs[isdir.(subdirs)]
	ndirs = length(subdirs)
	if ndirs != length(set_biases)
		println("Error: input folder is incomplete or step_bias does not match\n")
		exit()
	end

	# read configs that remain constant over all batches of simulations 
	global config_ = load(joinpath(subdirs[2],"config.jld2"))	
	global n_simulations = config_["n_simulations"]
	global n_states = config_["n_states"]
	global n_messages = config_["n_messages"]
	global n_actions = config_["n_actions"]
	global T = collect(0:1f0/(n_states-1):1) 
	global A = collect(0:1f0/(n_actions-1):1)
	global loss_type = config_["loss"]
	global dist_type = config_["dist"]
	global p_t = gen_distribution()
	global temp0_s = Float32(config_["temp0_s"])
	global temp0_r = Float32(config_["temp0_r"])
	global lambda_s = Float32(config_["lambda_s"])
	global lambda_r = Float32(config_["lambda_r"])

	extracted_data = DefaultDict(() -> Array{Any,1}(undef, length(set_biases)))
	counter = 0
	for subdir in subdirs
		isdir(subdir) || continue
	    print("\rAnalyzing data $(counter+=1)/$ndirs")
	    flush(stdout)
		config = load(joinpath(subdir,"config.jld2"))
		results = load(joinpath(subdir,"results.jld2"))
		extract_data(config, results, extracted_data)
	end
	println()
	return config_, extracted_data
end
