# This reads the simulations outcomes generated by 1_script.jl
# this file is loaded by 1_plots.jl

# load dependencies
using TOML
using JLD2
using StatsBase
using DataStructures
using LoopVectorization
using Random
using ArgParse

# load files and functions 
macro names(arg...) string.(arg) end
include(joinpath(pwd(),"analysis.jl"))
include(joinpath(pwd(),"rl_agents.jl"))
include(joinpath(pwd(),"nash.jl"))

# functions

function parse_commandline()
    arg_settings = ArgParseSettings(allow_ambiguous_opts=true)
    @add_arg_table! arg_settings begin
        # GAME SETTINGS
        "--in_dir", "-i"
            arg_type = String
            help = "input directory"
            default = "out_basecase"
        "--step_bias"
            arg_type = Float32
            help = "space between points in [0.0,0.5]"
            default = 0.005f0
    end
    parsed_args = parse_args(arg_settings)
    return parsed_args
end

function extract_data(config, results, babbling_nash, extracted_data)
	# extract and process data from data structures

	# read from files, some variables must be global
	global bias = config["bias"]
	global reward_matrix_s, reward_matrix_r = k .* gen_reward_matrix()

	babbling_action = argmax(reward_matrix_r*p_t)
	babbling_reward_s::Float32 = p_t'reward_matrix_s[babbling_action,:]
	babbling_reward_r::Float32 = p_t'reward_matrix_r[babbling_action,:]

	n_max_episodes = config["n_max_episodes"] 	
	Q_s = results["Q_s"]
	Q_r = results["Q_r"]
	n_episodes = results["n_episodes"]

	# compute is_converged bool and frequence
    is_converged = n_episodes .< n_max_episodes
    n_converged = count(is_converged)

	# preallocate array
    expected_reward_s = Array{Float32,1}(undef, n_converged)
    expected_reward_r = Array{Float32,1}(undef, n_converged)
    expected_aggregate_reward = Array{Float32,1}(undef, n_converged)
    absolute_error_s = Array{Float32,1}(undef, n_converged)
    absolute_error_r = Array{Float32,1}(undef, n_converged)
    max_absolute_error = Array{Float32,1}(undef, n_converged)
    mutual_information = Array{Float32,1}(undef, n_converged)
    n_on_path_messages = Array{Int64,1}(undef, n_converged)
    max_mass_on_suboptim_s = Array{Float32,1}(undef, n_converged)
    max_mass_on_suboptim_r = Array{Float32,1}(undef, n_converged)
    is_partitional = Array{Bool,1}(undef, n_converged)
    n_effective_messages = Array{Int64,1}(undef, n_converged)
    max_max_mass_on_suboptim = Array{Float32,1}(undef, n_converged)
	# compute metrics of interest for converged sessions
	Threads.@threads for z in 1:n_converged
		is_converged[z] == true || continue
        # get policies at convergence
       	policy_s = get_policy(Q_s[:,:,z], max(temp0_s*lambda_s^(n_episodes[z]-1),1f-30))
        policy_r = get_policy(Q_r[:,:,z], max(temp0_r*lambda_r^(n_episodes[z]-1),1f-30))
        # compute (ex-ante) expected rewards at convergence
        expected_reward_s[z], expected_reward_r[z] = get_expected_rewards(policy_s, policy_r)
        # compute best response to opponent's policy at convergence
        optimal_policy_s = get_best_reply_s(policy_r)
        optimal_policy_r = get_best_reply_r(policy_s)
        # compute expected rewards by best responding to opponent
        optimal_reward_s, _ = get_expected_rewards(optimal_policy_s, policy_r)
        _, optimal_reward_r = get_expected_rewards(policy_s, optimal_policy_r)
        # compute absolute expected error by (possibly) not best responding to opponent
        absolute_error_s[z] = optimal_reward_s - expected_reward_s[z] 
        absolute_error_r[z] = optimal_reward_r - expected_reward_r[z]
		max_absolute_error[z] = max(absolute_error_s[z], absolute_error_r[z])
        # compute communication metrics
        mutual_information[z] = get_mutual_information(policy_s)
        # get on path messages
        off_path_messages = get_off_path_messages(policy_s)
		n_on_path_messages[z] = n_messages - count(off_path_messages)
        # compute maximum mass on suboptim messages (actions) across states (messages)
        mass_on_suboptim_s, mass_on_suboptim_r = get_mass_on_suboptim(policy_s, policy_r, optimal_policy_s, optimal_policy_r)
        max_mass_on_suboptim_s[z] = maximum(mass_on_suboptim_s)
    	max_mass_on_suboptim_r[z] = maximum(mass_on_suboptim_r[.!off_path_messages])
		# check if is a nash
        max_max_mass_on_suboptim[z] = max(max_mass_on_suboptim_s[z], max_mass_on_suboptim_r[z])
        # check if policy is partitional
        is_partitional[z] = ispartitional(policy_s)
        # count number of messages that have no synonyms
        n_effective_messages[z] = get_n_effective_messages(policy_s[:,.!off_path_messages])
    end

	i = findfirst(set_biases .== bias)
	extracted_data["n_episodes"][:,i] .= n_episodes
	extracted_data["is_converged"][:,i] .= is_converged
	extracted_data["expected_reward_s"][:,i] .= expected_reward_s
	extracted_data["expected_reward_r"][:,i] .= expected_reward_r
	extracted_data["mutual_information"][:,i] .= mutual_information
	extracted_data["absolute_error_s"][:,i] .= absolute_error_s
	extracted_data["absolute_error_r"][:,i] .= absolute_error_r
	extracted_data["max_absolute_error"][:,i] .= max_absolute_error
	extracted_data["n_on_path_messages"][:,i] .= n_on_path_messages
	extracted_data["max_mass_on_suboptim_s"][:,i] .= max_mass_on_suboptim_s
	extracted_data["max_mass_on_suboptim_r"][:,i] .= max_mass_on_suboptim_r
	extracted_data["max_max_mass_on_suboptim"][:,i] .= max_max_mass_on_suboptim
	extracted_data["is_partitional"][:,i] .= is_partitional
	extracted_data["n_effective_messages"][:,i] .= n_effective_messages
	babbling_nash["babbling_reward_s"][i] = babbling_reward_s
	babbling_nash["babbling_reward_r"][i] = babbling_reward_r
end

function get_best_nash_equilibria(n_steps)
	# Compute sender-prefered equilibria for bias in range(0,0.5,n_steps)
	# only supports n_messages = n_states
	best_nash = DefaultDict(() -> Array{Float32,1}(undef,n_steps))
	set_biases_ = range(0,0.5,n_steps)
	for i in 1:n_steps
	    print("\rComputing benchmark $i/$n_steps")
	    flush(stdout)
		global bias = set_biases_[i]
		global reward_matrix_s, reward_matrix_r = k .* gen_reward_matrix()
		best_nash_i = get_best_nash()
		best_nash["best_expected_reward_s"][i] = best_nash_i["best_expected_reward_s"]
		best_nash["best_expected_reward_r"][i] = best_nash_i["best_expected_reward_r"]
		best_nash["best_mutual_information"][i] = best_nash_i["best_mutual_information"]
		best_nash["n_messages_on_path"][i] = best_nash_i["n_messages_on_path"]
	end
	println()
	return best_nash
end

# parse terminal config
const scrpt_config = parse_commandline()
# define set of biases 
const set_biases = Float32.(collect(0.00f0:scrpt_config["step_bias"]:0.5f0))
# define input dir 
const input_dir = joinpath(pwd(), scrpt_config["in_dir"])

# navigate to relevant subdir 
dirs = readdir(input_dir, join = true)
dirs = dirs[isdir.(dirs)]
dirs = setdiff(dirs,joinpath.(input_dir,["pdf", "temp", "tikz"]))
dir_id = 1
if length(dirs) > 1
	println("list of directories: ")
	for (dir_id, dir) in enumerate(dirs)
		println(dir_id,": ", split(dir,"/")[end])
	end
	while true
		print("select directory: ")
		global dir_id = parse(Int, readline())
		dir_id in keys(dirs) && break
	end
end
dir = dirs[dir_id]
const subdirs = readdir(dir, join=true)

# read configs that remain constant over all batches of simulations 
const config_ = load(joinpath(subdirs[2],"config.jld2"))	
const n_simulations = config_["n_simulations"]
const n_states = config_["n_states"]
const n_messages = config_["n_messages"]
const n_actions = config_["n_actions"]
const T = collect(0:1f0/(n_states-1):1) 
const A = collect(0:1f0/(n_actions-1):1)
const loss_type = config_["loss"]
const k::Float32 = config_["factor"]
const dist_type = config_["dist"]
const p_t = gen_distribution()
const temp0_s::Float32 = config_["temp0_s"]
const temp0_r::Float32 = config_["temp0_r"]
const lambda_s::Float32 = config_["lambda_s"]
const lambda_r::Float32 = config_["lambda_r"]

println("\nInput dir: ", scrpt_config["in_dir"])

function read_data()
	# main loop, read files in subdirs, extract and process data, save to extracted_data
	babbling_nash = DefaultDict(() -> Array{Any,1}(undef, length(set_biases)))
	extracted_data = DefaultDict(() -> Array{Any,2}(undef, n_simulations, length(set_biases)))
	counter = 0
	ndirs = count(isdir.(subdirs))
	if ndirs != length(set_biases)
		println("Error: incomplete input")
		exit()
	end
	for subdir in subdirs
		isdir(subdir) || continue
	    print("\rAnalyzing data $(counter+=1)/$ndirs")
	    flush(stdout)
		config = load(joinpath(subdir,"config.jld2"))
		results = load(joinpath(subdir,"results.jld2"))
		extract_data(config, results, babbling_nash, extracted_data)
	end
	println()
	best_nash = get_best_nash_equilibria(1001)
	return extracted_data, best_nash, babbling_nash
end
