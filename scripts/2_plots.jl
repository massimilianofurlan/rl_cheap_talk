# This script generates plots from the simulations outcomes generated by 2_script.jl
# Usage:
# 1) navigate to the project directory 'rl_cheap_talk' 
# 2) run 'julia --threads NUM_THREADS scripts/2_plots.jl -i INPUT_DIR'
#	 replacing NUM_THREADS with the desired number of threads 
#    and INPUT_DIR with the output directory of 2_script.jl 

# load dependencies
using TOML
using JLD2
using StatsBase
using DataStructures
using PGFPlotsX
using LoopVectorization
using Random
using Colors
using ArgParse

# load files
macro names(arg...) string.(arg) end
include(joinpath(pwd(),"analysis.jl"))
include(joinpath(pwd(),"rl_agents.jl"))

# functions

function parse_commandline()
    arg_settings = ArgParseSettings(allow_ambiguous_opts=true)
    @add_arg_table! arg_settings begin
        # GAME SETTINGS
        "--in_dir", "-i"
            arg_type = String
            help = "input directory"
			default = "out_grid_search"
        "--step_bias"
            arg_type = Float32
            help = "space between points in [0.0,0.5]"
            default = 0.01f0
    end
    parsed_args = parse_args(arg_settings)
    return parsed_args
end


function extract_data(config, results, best_nash, extracted_data)
	# extract and process data from data structures

	# read from files, some variavles must be global
	global bias = config["bias"]
	global reward_matrix_s, reward_matrix_r = k .* gen_reward_matrix()
	lambda_s::Float32 = config["lambda_s"]
	lambda_r::Float32 = config["lambda_r"]
	alpha_s::Float32 = config["alpha_s"]
	alpha_r::Float32 = config["alpha_r"]
	temp0_s::Float32 = config["temp0_s"]
	temp0_r::Float32 = config["temp0_r"]
	babbling_action = argmax(reward_matrix_r*p_t)
	babbling_reward_s::Float32 = p_t'reward_matrix_s[babbling_action,:]
	babbling_reward_r::Float32 = p_t'reward_matrix_r[babbling_action,:]
	babbling_aggregate_reward = babbling_reward_s + babbling_reward_r

	n_max_episodes = config["n_max_episodes"] 	
	Q_s = results["Q_s"]
	Q_r = results["Q_r"]
	n_episodes = results["n_episodes"]
	best_expected_reward_s = best_nash["best_expected_reward_s"]
	best_expected_reward_r = best_nash["best_expected_reward_r"]
	best_expected_aggregate_reward = best_expected_reward_s + best_expected_reward_r
	best_mutual_information = best_nash["best_mutual_information"]

	# print n_states and bias to console
	println("n_states: ", n_states, "\tbias: ", bias, "\talpha: ", alpha_s, "\tlambda_s: ", lambda_s)

	# compute is_converged bool and frequence
    is_converged = n_episodes .< n_max_episodes
    n_converged = count(is_converged)
	freq_converged = n_converged / n_simulations

	# preallocate array
    expected_reward_s = Array{Float32,1}(undef, n_converged)
    expected_reward_r = Array{Float32,1}(undef, n_converged)
    expected_aggregate_reward = Array{Float32,1}(undef, n_converged)
    absolute_error_s = Array{Float32,1}(undef, n_converged)
    absolute_error_r = Array{Float32,1}(undef, n_converged)
    max_absolute_error = Array{Float32,1}(undef, n_converged)
    mutual_information = Array{Float32,1}(undef, n_converged)
	# compute metrics of interest for converged sessions
	Threads.@threads for z in 1:n_converged
		is_converged[z] == true || continue
        # get policies at convergence
       	policy_s = get_policy(Q_s[:,:,z], temp0_s*lambda_s^(n_episodes[z]-1))
        policy_r = get_policy(Q_r[:,:,z], temp0_r*lambda_r^(n_episodes[z]-1))
        # compute (ex-ante) expected rewards at convergence
        expected_reward_s[z], expected_reward_r[z] = get_expected_rewards(policy_s, policy_r)
        expected_aggregate_reward[z] = expected_reward_s[z] + expected_reward_r[z]  
        # compute best response to opponent's policy at convergence
        optimal_policy_s = get_best_reply_s(policy_r)
        optimal_policy_r = get_best_reply_r(policy_s)
        # compute expected rewards by best responding to opponent
        optimal_reward_s, _ = get_expected_rewards(optimal_policy_s, policy_r)
        _, optimal_reward_r = get_expected_rewards(policy_s, optimal_policy_r)
        # compute absolute expected error by (possibly) not best responding to opponent
        absolute_error_s[z] = optimal_reward_s - expected_reward_s[z] 
        absolute_error_r[z] = optimal_reward_r - expected_reward_r[z]
		max_absolute_error[z] = max(absolute_error_s[z], absolute_error_r[z])
        # compute communication metrics
        mutual_information[z] = get_mutual_information(policy_s)
    end

	mean_quant(x,α) = mean(x), quantile(x, [α/2, 1-α/2])
	avg_expected_reward_s = mean_quant(expected_reward_s,1.0-α)
	avg_expected_reward_r = mean_quant(expected_reward_r,1.0-α)
	avg_expected_aggregate_reward = mean_quant(expected_aggregate_reward,1.0-α)
	avg_mutual_information = mean_quant(mutual_information,1.0-α)
	avg_absolute_error_s = mean_quant(absolute_error_s,1.0-α)
	avg_absolute_error_r = mean_quant(absolute_error_r,1.0-α)
	avg_n_episodes = mean_quant(n_episodes,1.0-α)
	q_max_absolute_error = quantile(max_absolute_error, β)

	i = findfirst(set_biases .== bias)
	extracted_data["avg_n_episodes"][i] = avg_n_episodes
	extracted_data["freq_converged"][i] = freq_converged
	extracted_data["avg_expected_reward_s"][i] = avg_expected_reward_s
	extracted_data["avg_expected_reward_r"][i] = avg_expected_reward_r
	extracted_data["avg_expected_aggregate_reward"][i] = avg_expected_aggregate_reward
	extracted_data["avg_mutual_information"][i] = avg_mutual_information
	extracted_data["best_expected_reward_s"][i] = best_expected_reward_s
	extracted_data["best_expected_reward_r"][i] = best_expected_reward_r
	extracted_data["best_expected_aggregate_reward"][i] = best_expected_aggregate_reward
	extracted_data["best_mutual_information"][i] = best_mutual_information
	extracted_data["avg_absolute_error_s"][i] = avg_absolute_error_s
	extracted_data["avg_absolute_error_r"][i] = avg_absolute_error_r
	extracted_data["q_max_absolute_error"][i] = q_max_absolute_error
	extracted_data["babbling_reward_s"][i] = babbling_reward_s
	extracted_data["babbling_reward_r"][i] = babbling_reward_r
	extracted_data["babbling_aggregate_reward"][i] = babbling_aggregate_reward
	return alpha_s, lambda_s
end


function gen_empty_plot(;title = "", ylabel = "", ymin = nothing, ymax = nothing, legend_pos = "")
   	# generate empty plot with predefined axis 

    xtixkstep= trunc(Int,div(0.1,scrpt_config["step_bias"]))
    xticks = 1:xtixkstep:length(set_biases)
    n_biases = length(set_biases)

    pl = @pgf Axis(
        {   xlabel = raw"$b$", 
            ylabel = ylabel,
            xmin = 1, 
            xmax = n_biases,
            ymin = ymin,
            ymax = ymax, 
            title = title,
            xtick = xticks,
            xticklabels = string.(set_biases[xticks]),
            raw"y tick label style={/pgf/number format/fixed}",
            width = raw"{0.45\linewidth}",
            height= raw"{0.40\linewidth}",
            raw"xlabel style={name=xlabel}", 
        }
    );
 	legend_pos == "out_bottom" && push!(pl.options, raw"xlabel style={name=xlabel}")
    legend_pos == "out_bottom" && push!(pl.options, raw"legend style={at={(xlabel.south)},anchor=north,legend columns = -1, column sep = 5pt}")
	return pl
end


function plot_avg!(pl, data; legend = "", color = "red")
	# plot average value over all simulations and confidence interval (adds on top to existing plot)
	average = getindex.(data, 1)
    confidence_interval = hcat(getindex.(data, 2)...)'

    n_biases = length(set_biases)

    @pgf pl_avg = Plot({color = color}, Table(x = 1:n_biases, y = average));
	@pgf pl_ub = Plot({"name path=f", no_marks, thin, "draw=none", "forget plot"}, Table(x = 1:n_biases, y = confidence_interval[:,1]));
	@pgf pl_lb = Plot({"name path=g", no_marks, thin, "draw=none", "forget plot"}, Table(x = 1:n_biases, y = confidence_interval[:,2]));
	@pgf pl_fill = Plot({color = color, fill = color, opacity = 0.1, "forget plot"}, raw"fill between [of=f and g]");

	push!(pl, pl_avg);
	push!(pl, pl_ub);
	push!(pl, pl_lb);
	push!(pl, pl_fill);

    !isempty(legend) && push!(pl, LegendEntry(legend))

	return pl
end


function plot_val!(pl, data; legend = "", color = "red", style = "solid", opacity = 1.0, forget = false, legend_pos = "")
	# plot value on top of existing plot
	val = getindex.(data,1)
    @pgf pl_val = Plot({color = color, style = style, opacity = opacity}, Table(x = 1:length(set_biases), y = val));
   	forget && push!(pl_val.options,"forget plot")
	push!(pl, pl_val);
	!isempty(legend) && push!(pl, LegendEntry(legend))
	return pl
end


function plot_eq_bound!(pl, best_mutual_information)
	# plot shaded areas to indicate where babbling is the unique equilibrium and where full communication is an equilibrium
	# last index at which mutual information is 1.0
	mi1_idx = findlast(best_mutual_information .== maximum(best_mutual_information))
	# first index at which mutual information is 0.0
	mi0_idx = findfirst(best_mutual_information .== minimum(best_mutual_information))
	@pgf pl_best_lx_bound = VLine({"draw=none", "name path=blx"}, 1)
	@pgf pl_best_rx_bound = VLine({"draw=none", "name path=brx"}, mi1_idx)
	@pgf pl_best_fill =  Plot({color = "gray", fill = "gray", opacity = 0.08}, raw"fill between [of=blx and brx]");
	@pgf pl_worst_lx_bound =  VLine({"draw=none", "name path=wlx"}, mi0_idx)
	@pgf pl_worst_rx_bound = VLine({"draw=none", "name path=wrx"}, length(set_biases))
	@pgf pl_worst_fill =  Plot({color = "gray", fill = "gray", opacity = 0.08}, raw"fill between [of=wlx and wrx]");
	push!(pl, pl_best_lx_bound);
	push!(pl, pl_best_rx_bound);
	push!(pl, pl_best_fill);
	push!(pl, pl_worst_lx_bound);
	push!(pl, pl_worst_rx_bound);
	push!(pl, pl_worst_fill);
end


function plot_q_abs_error_hm(matrix_data)
	# heatmap of absolute errors out of matrix_data
	x = repeat(1:length(set_lambda), outer = length(set_lambda))
	y = repeat(1:length(set_alpha), inner = length(set_alpha))
	coord = Coordinates(x, y; meta = vec(matrix_data'[:,end:-1:1]))
	axis = @pgf Axis(
	    {
	    	xlabel= raw"$\lambda$",
	    	ylabel= raw"$\alpha$", 
	        enlargelimits = false,
	        xtick = (1:length(set_lambda)).-0.5,
	        ytick = 1:length(set_alpha),
	  	    xticklabels = string.(set_lambda),
	        yticklabels = string.(set_alpha)[end:-1:1],
	        "point meta min=0.003",
			"point meta max=0.015",
	        "colorbar",
	        "colormap={reversed blackwhite}{gray(0cm)=(1); gray(1cm)=(0)}",
	        #"colormap/blackwhite",
	        #"colorbar style={scaled ticks=false, /pgf/number format/fixed, /pgf/number format/precision =5}",
	        #"colormap={jet}{rgb255(0cm)=(128,0,0) rgb255(1cm)=(255,0,0) rgb255(3cm)=(255,255,0) rgb255(5cm)=(0,255,255) rgb255(7cm)=(0,0,255) rgb255(8cm)=(0,0,128)}",
	        "colorbar sampled",
	        "colormap={CM}{samples of colormap=(12 of reversed blackwhite)}", 
	        "colormap access=piecewise constant",
	        raw"colorbar style={scaled ticks=false, /pgf/number format/fixed, /pgf/number format/precision =5, ytick={0.003, 0.005, 0.007, 0.009, 0.011, 0.013, 0.015}, ytick={0.003, 0.005, 0.007, 0.009, 0.011, 0.013, 0.015}, ytick={0.003, 0.005, 0.007, 0.009, 0.011, 0.013, 0.015}, yticklabels={0.003, 0.005, 0.007, 0.009, 0.011, 0.013, $>$0.015},samples = 13}",
	        "xticklabel style={rotate=45}"
	    },
	    PlotInc(
	        {
	            matrix_plot,
	            mark = "",
	            point_meta = "explicit",
	            "mesh/cols" = length(set_lambda)
	        },
	        coord,
	    ),
	)
end


function plot_avg_n_eps_hm(matrix_data)
	# heatmap of n_episodes out of matrix data
	x = repeat(1:length(set_lambda), outer = length(set_lambda))
	y = repeat(1:length(set_alpha), inner = length(set_alpha))
	coord = Coordinates(x, y; meta = vec(matrix_data'[:,end:-1:1]))
	axis = @pgf Axis(
	    {
	    	xlabel= raw"$\lambda$",
	    	ylabel= raw"$\alpha$", 
	        enlargelimits = false,
	        xtick = (1:length(set_lambda)).-0.5,
	        ytick = 1:length(set_alpha),
	  	    xticklabels = string.(set_lambda),
	        yticklabels = string.(set_alpha)[end:-1:1],
			"point meta max=5000000",
			"point meta min=0",
	        "colorbar",
	        "colormap/blackwhite",
	        #"colorbar style={scaled ticks=false, /pgf/number format/fixed, /pgf/number format/precision =5}",
	        #"colormap={jet}{rgb255(0cm)=(128,0,0) rgb255(1cm)=(255,0,0) rgb255(3cm)=(255,255,0) rgb255(5cm)=(0,255,255) rgb255(7cm)=(0,0,255) rgb255(8cm)=(0,0,128)}",
	        "colorbar sampled",
	        "colormap={CM}{samples of colormap=(10 of blackwhite)}", 
	        "colormap access=piecewise constant",
	        "colorbar style={scaled ticks=false, /pgf/number format/fixed, /pgf/number format/precision =5, samples = 11}",
	        "xticklabel style={rotate=45}"
	    },
	    PlotInc(
	        {
	            matrix_plot,
	            mark = "",
	            point_meta = "explicit",
	            "mesh/cols" = length(set_lambda)
	        },
	        coord,
	    ),
	)
end


# main 

# parse terminal config
const scrpt_config = merge(parse_commandline(),TOML.parsefile("scripts/2_config.toml"))
# define set of biases 
const set_biases = Float32.(collect(0.00f0:scrpt_config["step_bias"]:0.5f0))
# define input dir 
const input_dir = joinpath(pwd(), scrpt_config["in_dir"])

# navigate to a subdir to get configs
dirs = readdir(input_dir, join = true)
dirs = dirs[isdir.(dirs)]
dirs = setdiff(dirs,joinpath.(input_dir,["pdf", "temp", "tikz"]))
subdir = readdir(dirs[2], join=true)[2]

const config_ = load(joinpath(subdir,"config.jld2"))	
const n_simulations = config_["n_simulations"]
const n_states = config_["n_states"]
const n_actions = config_["n_actions"]
const n_messages = config_["n_messages"]
const T = collect(0:1f0/(n_states-1):1) 
const A = collect(0:1f0/(n_actions-1):1)
const loss_type = config_["loss"]
const k::Float32 = config_["factor"]
const dist_type = config_["dist"]
const p_t = gen_distribution()
const temp0_s::Float32 = config_["temp0_s"]
const temp0_r::Float32 = config_["temp0_r"]

# set of alphas to loop over (linearly spaced)
const set_alpha = Float32.(range(scrpt_config["min_alpha"],scrpt_config["max_alpha"],scrpt_config["n_alpha"]))
# construct set of lambdas to loop over (space linearly the n of episodes to reach lambda = 0.01)
const lin_range_n_ep = range(log(0.01)/log(scrpt_config["min_lambda"]),log(0.01)/log(scrpt_config["max_lambda"]),scrpt_config["n_lambda"])
const set_lambda = Float32.(round.(0.01.^ (1 ./ lin_range_n_ep), digits = 8))


# percent of simulation outcomes to fall into confidence interval
const α = 0.95
# quantile for epsilon nash equilibria
const β = 0.9 

# preallocated matrices -> heatmaps 
q_absolute_error = ones(length(set_alpha),length(set_lambda)) 
avg_n_episodes = ones(length(set_alpha),length(set_lambda)) 

# GENERATE PLOTS
# generate empty plots
push!(PGFPlotsX.CUSTOM_PREAMBLE, raw"\usepgfplotslibrary{fillbetween}")
pl_mutual_information = gen_empty_plot(title = "normalized mutual information", legend_pos = "out_bottom");
pl_expected_reward_s = gen_empty_plot(title = "ex-ante expected reward (sender)", legend_pos = "out_bottom");
pl_expected_reward_r =  gen_empty_plot(title = "ex-ante expected reward (receiver)", legend_pos = "out_bottom");
group_pl_expected_reward_s =  gen_empty_plot(title = "ex-ante expected reward (sender)");
group_pl_expected_reward_r =  gen_empty_plot(title = "ex-ante expected reward (receiver)");
pl_absolute_error = gen_empty_plot(title = "absolute error",legend_pos = "out_bottom");

# overlay plots
for dir in readdir(input_dir, join = true)
	isdir(dir) || continue
	length(readdir(dir)) >= length(set_biases) || continue
	extracted_data = DefaultDict(() -> Array{Any,1}(undef,length(set_biases)))
	for subdir in readdir(dir, join=true)
		isdir(subdir) || continue
		config = load(joinpath(subdir,"config.jld2"))
		results = load(joinpath(subdir,"results.jld2"))
		best_nash = load(joinpath(subdir,"best_nash.jld2"))
		global alpha, lambda = extract_data(config, results, best_nash, extracted_data)
	end
	extracted_data["best_reply_expected_reward_s"] = getindex.(extracted_data["avg_expected_reward_s"],1) .- getindex.(extracted_data["avg_absolute_error_s"],1)
	extracted_data["best_reply_expected_reward_r"] = getindex.(extracted_data["avg_expected_reward_r"],1) .- getindex.(extracted_data["avg_absolute_error_r"],1)

	i = findfirst(set_alpha .== alpha)
	j = findfirst(set_lambda .== lambda)
	q_absolute_error[i,j] = maximum(extracted_data["q_max_absolute_error"])
	avg_n_episodes[i,j] = mean(getindex.(extracted_data["avg_n_episodes"],1))

	last_plot = i == length(set_alpha) && j == length(set_lambda)
	
	# mutual information
	global pl_mutual_information = plot_val!(pl_mutual_information, getindex.(extracted_data["avg_mutual_information"],1), color=range(RGB(0.0,0.70,1.0), stop=RGB(0.0,0.0,1.0), length=10)[j], opacity = 0.2, forget = !last_plot);
	#rewards, separate plots
	global pl_expected_reward_s = plot_val!(pl_expected_reward_s, getindex.(extracted_data["avg_expected_reward_s"],1), color=range(RGB(0.0,0.70,1.0), stop=RGB(0.0,0.0,1.0), length=10)[j], opacity = 0.2, forget = !last_plot);
	global pl_expected_reward_r = plot_val!(pl_expected_reward_r, getindex.(extracted_data["avg_expected_reward_r"],1), color=range(RGB(0.0,0.70,1.0), stop=RGB(0.0,0.0,1.0), length=10)[j], opacity = 0.2, forget = !last_plot);
	# absolute error
	global pl_absolute_error = plot_val!(pl_absolute_error, getindex.(extracted_data["avg_absolute_error_s"],1), color="red", opacity = 0.5*j, forget = !last_plot);
	global pl_absolute_error = plot_val!(pl_absolute_error, getindex.(extracted_data["avg_absolute_error_r"],1), color="blue", opacity = 0.05*j, forget = !last_plot);
	# group plot of rewards for grid search 
	global group_pl_expected_reward_s = plot_val!(group_pl_expected_reward_s, getindex.(extracted_data["avg_expected_reward_s"],1), color=range(RGB(0.0,0.70,1.0), stop=RGB(0.0,0.0,1.0), length=10)[j], opacity = 0.2, forget = true);
	global group_pl_expected_reward_r = plot_val!(group_pl_expected_reward_r, getindex.(extracted_data["avg_expected_reward_r"],1), color=range(RGB(0.0,0.70,1.0), stop=RGB(0.0,0.0,1.0), length=10)[j], opacity = 0.2, forget = !last_plot);
	if last_plot
		# stuff to do only once, at the last iteration
		# add optimal equilibrium 
		global pl_mutual_information = plot_val!(pl_mutual_information, extracted_data["best_mutual_information"], color = "red");
		global babbling_mutual_information = fill(extracted_data["best_mutual_information"][end],length(set_biases))
		global pl_mutual_information = plot_val!(pl_mutual_information, babbling_mutual_information; color = "darkgray", style = "dotted");
		global pl_expected_reward_s = plot_val!(pl_expected_reward_s, extracted_data["best_expected_reward_s"], color = "red");
		global pl_expected_reward_s = plot_val!(pl_expected_reward_s, extracted_data["babbling_reward_s"]; color="darkgray", style = "dotted");
		global pl_expected_reward_r = plot_val!(pl_expected_reward_r, extracted_data["best_expected_reward_r"], color = "red");
		global pl_expected_reward_r = plot_val!(pl_expected_reward_r, extracted_data["babbling_reward_r"], color="darkgray", style = "dotted");
		global group_pl_expected_reward_s = plot_val!(group_pl_expected_reward_s, extracted_data["best_expected_reward_s"], color = "red", forget = true);
		global group_pl_expected_reward_s = plot_val!(group_pl_expected_reward_s, extracted_data["babbling_reward_s"]; color="darkgray", style = "dotted");		
		global group_pl_expected_reward_r = plot_val!(group_pl_expected_reward_r, extracted_data["best_expected_reward_r"], color = "red");	
		global group_pl_expected_reward_r = plot_val!(group_pl_expected_reward_r, extracted_data["babbling_reward_r"], color="darkgray", style = "dotted");
		# add legends
		push!(pl_mutual_information, LegendEntry("simulations"))
		push!(pl_mutual_information, LegendEntry("optimal equilibrium"))
		push!(pl_mutual_information, LegendEntry("babbling equilibrium"))
		push!(pl_expected_reward_s, LegendEntry("simulations"))
		push!(pl_expected_reward_s, LegendEntry("optimal equilibrium"))
		push!(pl_expected_reward_s, LegendEntry("babbling equilibrium"))
		push!(pl_expected_reward_r, LegendEntry("simulations"))
		push!(pl_expected_reward_r, LegendEntry("optimal equilibrium"))
		push!(pl_expected_reward_r, LegendEntry("babbling equilibrium"))
		push!(pl_absolute_error, LegendEntry("sender"))
		push!(pl_absolute_error, LegendEntry("receiver"))
		push!(group_pl_expected_reward_r.options, raw"legend style={legend columns = -1, legend to name={legend_grid_search_group_expected_rewards}, column sep = 5pt}")
		push!(group_pl_expected_reward_r, LegendEntry("simulations"))
		push!(group_pl_expected_reward_r, LegendEntry("optimal equilibrium"))
		push!(group_pl_expected_reward_r, LegendEntry("babbling equilibrium"))
		# add equilibria bound
		global group_pl_expected_reward_s = plot_eq_bound!(group_pl_expected_reward_s,extracted_data["best_mutual_information"])
		global group_pl_expected_reward_r = plot_eq_bound!(group_pl_expected_reward_r,extracted_data["best_mutual_information"])
	end
end

hm_q_absolute_error = plot_q_abs_error_hm(q_absolute_error);
hm_avg_n_episodes = plot_avg_n_eps_hm(avg_n_episodes);

group_pl_expected_rewards = @pgf GroupPlot(
							{ group_style = { group_size="2 by 1", raw"horizontal sep = 50pt" },
   							 }, group_pl_expected_reward_s, group_pl_expected_reward_r);


pdf_dir = mkpath(joinpath(input_dir,"pdf"))
tikz_dir = mkpath(joinpath(input_dir,"tikz"))
pgfsave(joinpath(tikz_dir,"grid_search_expected_reward_s.tikz"),pl_expected_reward_s)
pgfsave(joinpath(pdf_dir,"grid_search_expected_reward_s.pdf"),pl_expected_reward_s)
pgfsave(joinpath(tikz_dir,"grid_search_expected_reward_r.tikz"),pl_expected_reward_r)
pgfsave(joinpath(pdf_dir,"grid_search_expected_reward_r.pdf"),pl_expected_reward_r)
pgfsave(joinpath(tikz_dir,"grid_search_mutual_information.tikz"),pl_mutual_information)
pgfsave(joinpath(pdf_dir,"grid_search_mutual_information.pdf"),pl_mutual_information)
pgfsave(joinpath(tikz_dir,"grid_search_absolute_error.tikz"),pl_absolute_error)
pgfsave(joinpath(pdf_dir,"grid_search_absolute_error.pdf"),pl_absolute_error)
pgfsave(joinpath(tikz_dir,"grid_search_hm_q_absolute_error.tikz"),hm_q_absolute_error)
pgfsave(joinpath(pdf_dir,"grid_search_hm_q_absolute_error.pdf"),hm_q_absolute_error)
pgfsave(joinpath(tikz_dir,"grid_search_hm_avg_n_episodes.tikz"),hm_avg_n_episodes)
pgfsave(joinpath(pdf_dir,"grid_search_hm_avg_n_episodes.pdf"),hm_avg_n_episodes)
pgfsave(joinpath(tikz_dir,"grid_search_group_expected_rewards.tikz"),group_pl_expected_rewards)
pgfsave(joinpath(pdf_dir,"grid_search_group_expected_rewards.pdf"),group_pl_expected_rewards)


